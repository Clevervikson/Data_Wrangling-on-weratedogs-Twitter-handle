{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "The dataset that I did my  wrangling (and analyzing and visualizing) is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. These ratings almost always have a denominator of 10. The numerators, though? Almost always greater than 10. 11/10, 12/10, 13/10, etc. Why? Because \"they're good dogs Brent.\" WeRateDogs has over 4 million followers and has received international media coverage.\n",
    "\n",
    "WeRateDogs downloaded their Twitter archive and sent it to Udacity via email exclusively for this analysis. This archive contains basic tweet data (tweet ID, timestamp, text, etc.) for all 5000+ of their tweets as they stood on August 1, 2017. More on this soon.\n",
    "\n",
    "\n",
    "The goal of this analysis is to wrangle WeRateDogs Twitter data to create interesting and trustworthy analyses and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Requirement\n",
    "\n",
    "Jupyter Notebook on your computer. The following packages (libraries) need to be installed. You can install these packages via conda or pip. \n",
    "- pandas\n",
    "\n",
    "\n",
    "- NumPy\n",
    "\n",
    "\n",
    "- requests\n",
    "\n",
    "\n",
    "- tweepy\n",
    "\n",
    "\n",
    "- json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling Process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering\n",
    "\n",
    "The following data were gathered for analysis:\n",
    "1. Twitter archive enhanced.\n",
    "\n",
    "The WeRateDogs Twitter archive contains basic tweet data for all 5000+ of their tweets, but not everything. One column the archive does contain though: each tweet's text, which I used to extract rating, dog name, and dog \"stage\" (i.e. doggo, floofer, pupper, and puppo) to make this Twitter archive \"enhanced.\"\n",
    "\n",
    "2. Image Predictions File\n",
    "\n",
    "This is a table full of image predictions (the top three only) alongside each tweet ID, image URL, and the image number that corresponded to the most confident prediction (numbered 1 to 4 since tweets can have up to four images). This is hosted on Udacity's servers and was downloaded programmatically using the Requests library and the following URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\n",
    "\n",
    "3. Additional Data via the Twitter API\n",
    "\n",
    "Retweet count and favorite count are two of the notable column omissions from the twitter enhanced archive. Fortunately, this additional data was gathered by anyone from Twitter's API. Using the tweet IDs in the WeRateDogs Twitter archive, I was able to query the Twitter API for each tweet's JSON data using Python's Tweepy library and store each tweet's entire set of JSON data in a file called tweet_json.txt file.\n",
    "\n",
    "Each tweet's JSON data was written to its own line. Then I read this .txt file line by line into a pandas DataFrame with tweet ID, retweet count, and favorite count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Assessing Data\n",
    "\n",
    "After gathering all three pieces of data, I was able to assess them visually and programmatically for quality and tidiness issues. I detected and documented at least twelve (12) quality issues and two (2) tidiness issues in this section.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Cleaning Data\n",
    "\n",
    "I cleaned all of the issues I documented while assessing. I first, made a copy of the original data. Then, I used the define-code-test framework and clearly document and clean each issues. Later on, I merged individual pieces of data according to the rules of tidy data which resulted into a high-quality and tidy master pandas DataFrame. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Storing Data\n",
    "\n",
    "In storing the data, I store the cleaned master DataFrame in a CSV file with the main one named twitter_archive_master.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analyzing and Visualizing Data\n",
    "\n",
    "I was able to produce three insights and one visualization. It was clearly documented and then the data was assessed and cleaned (if necessary) data used to make each analysis and visualization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
